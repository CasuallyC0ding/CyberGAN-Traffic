{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41395b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75adf6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading dataset...\n",
      "[INFO] Splitting IP addresses...\n",
      "[INFO] One-hot encoding categorical columns...\n",
      "[INFO] Scaling numerical features...\n",
      "[INFO] Saving processed dataset to processed_data.csv...\n",
      "[SUCCESS] Preprocessing complete. File saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Step 1: Load and Sample the Dataset ===\n",
    "INPUT_FILE = \"Dataset.csv\"\n",
    "OUTPUT_FILE = \"processed_data.csv\"\n",
    "SAMPLE_SIZE = 100_000\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"[INFO] Loading dataset...\")\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "df = df.sample(n=SAMPLE_SIZE, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# === Step 2: Split IP Addresses into Octets ===\n",
    "def split_ip_column(ip_series, prefix):\n",
    "    octets = ip_series.str.split('.', expand=True).astype(float)\n",
    "    octets.columns = [f\"{prefix}_octet{i+1}\" for i in range(4)]\n",
    "    return octets\n",
    "\n",
    "print(\"[INFO] Splitting IP addresses...\")\n",
    "df_orig_ip = split_ip_column(df[\"id.orig_h\"], \"orig_ip\")\n",
    "df_resp_ip = split_ip_column(df[\"id.resp_h\"], \"resp_ip\")\n",
    "\n",
    "df = pd.concat([df, df_orig_ip, df_resp_ip], axis=1)\n",
    "df.drop(columns=[\"id.orig_h\", \"id.resp_h\"], inplace=True)\n",
    "\n",
    "# === Step 3: One-Hot Encode Categorical Features ===\n",
    "print(\"[INFO] One-hot encoding categorical columns...\")\n",
    "categorical_cols = [\"proto\", \"conn_state\", \"history\", \"label\"]\n",
    "df = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "# === Step 4: Scale Numerical Features ===\n",
    "print(\"[INFO] Scaling numerical features...\")\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Identify numeric columns (all except one-hot encoded)\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# === Step 5: Save the Processed Dataset ===\n",
    "print(f\"[INFO] Saving processed dataset to {OUTPUT_FILE}...\")\n",
    "df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(\"[SUCCESS] Preprocessing complete. File saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3971b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading preprocessed data...\n",
      "Starting training...\n",
      "Epoch [50/500] Loss D: 1.1071, Loss G: 1.0009\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m real_data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Adversarial ground truths\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m real_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     82\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# ===== Train Discriminator =====\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "hidden_dim = 256\n",
    "batch_size = 64\n",
    "epochs = 300\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Model paths\n",
    "MODEL_DIR = \"saved_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "G_PATH = os.path.join(MODEL_DIR, \"generator.pth\")\n",
    "D_PATH = os.path.join(MODEL_DIR, \"discriminator.pth\")\n",
    "\n",
    "# Load and prepare your preprocessed data\n",
    "print(\"Loading preprocessed data...\")\n",
    "data = pd.read_csv(\"processed_data.csv\")\n",
    "data_values = data.values.astype(np.float32)\n",
    "tensor_data = torch.FloatTensor(data_values).to(device)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(tensor_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim * 4, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize networks\n",
    "output_dim = tensor_data.shape[1]\n",
    "generator = Generator(output_dim).to(device)\n",
    "discriminator = Discriminator(output_dim).to(device)\n",
    "\n",
    "# Try to load existing models\n",
    "try:\n",
    "    generator.load_state_dict(torch.load(G_PATH))\n",
    "    discriminator.load_state_dict(torch.load(D_PATH))\n",
    "    print(\"Loaded existing models\")\n",
    "except:\n",
    "    print(\"No saved models found, starting fresh\")\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training setup\n",
    "losses_g = []\n",
    "losses_d = []\n",
    "best_loss = float('inf')\n",
    "early_stop_patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop with progress bar\n",
    "try:\n",
    "    print(\"Starting training... Press Ctrl+C to save and exit early\")\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training GAN\"):\n",
    "        epoch_loss_g = 0\n",
    "        epoch_loss_d = 0\n",
    "        \n",
    "        for i, real_data in enumerate(dataloader):\n",
    "            real_data = real_data[0].to(device)\n",
    "            batch_size = real_data.size(0)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            discriminator.zero_grad()\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            # Real data\n",
    "            outputs_real = discriminator(real_data)\n",
    "            loss_real = criterion(outputs_real, real_labels)\n",
    "            \n",
    "            # Fake data\n",
    "            noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_data = generator(noise)\n",
    "            outputs_fake = discriminator(fake_data.detach())\n",
    "            loss_fake = criterion(outputs_fake, fake_labels)\n",
    "            \n",
    "            loss_D = loss_real + loss_fake\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            generator.zero_grad()\n",
    "            noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_data = generator(noise)\n",
    "            outputs = discriminator(fake_data)\n",
    "            loss_G = criterion(outputs, real_labels)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            epoch_loss_g += loss_G.item()\n",
    "            epoch_loss_d += loss_D.item()\n",
    "        \n",
    "        # Store average epoch losses\n",
    "        avg_loss_g = epoch_loss_g / len(dataloader)\n",
    "        avg_loss_d = epoch_loss_d / len(dataloader)\n",
    "        losses_g.append(avg_loss_g)\n",
    "        losses_d.append(avg_loss_d)\n",
    "        \n",
    "        # Early stopping check\n",
    "        current_loss = avg_loss_g + avg_loss_d\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(generator.state_dict(), G_PATH)\n",
    "            torch.save(discriminator.state_dict(), D_PATH)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Generate samples periodically\n",
    "        if (epoch+1) % 50 == 0:\n",
    "            torch.save(generator.state_dict(), G_PATH)\n",
    "            torch.save(discriminator.state_dict(), D_PATH)\n",
    "            print(f\"\\nEpoch {epoch+1} - Saved models\")\n",
    "            \n",
    "            # Generate and show sample\n",
    "            sample = generate_samples(1)\n",
    "            print(\"Sample features:\", sample.iloc[0, :5].to_dict())  # Show first 5 features\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "\n",
    "# Final save\n",
    "torch.save(generator.state_dict(), G_PATH)\n",
    "torch.save(discriminator.state_dict(), D_PATH)\n",
    "print(\"Models saved\")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_g, label='Generator Loss')\n",
    "plt.plot(losses_d, label='Discriminator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('training_loss.png')\n",
    "print(\"Training plot saved to training_loss.png\")\n",
    "\n",
    "# Generate final samples\n",
    "def generate_samples(num_samples):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_samples, latent_dim).to(device)\n",
    "        synthetic_data = generator(noise).cpu().numpy()\n",
    "    return pd.DataFrame(synthetic_data, columns=data.columns)\n",
    "\n",
    "synthetic_traffic = generate_samples(1000)\n",
    "synthetic_traffic.to_csv(\"synthetic_traffic.csv\", index=False)\n",
    "print(\"Generated samples saved to synthetic_traffic.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
